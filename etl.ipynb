{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek\n",
    "from pyspark.sql.functions import to_timestamp, date_format\n",
    "from pyspark.sql import types as t\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = 's3a://datalbc/input/'\n",
    "output_data = 's3a://datalbc/output/'\n",
    "#output_data = '/home/workspace/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#song_data = input_data + 'song-data/A/A/A/*.json'\n",
    "song_data = input_data + 'song-data/*/*/*/*.json'\n",
    "# read song data file\n",
    "df = spark.read.json(song_data)\n",
    "\n",
    "# extract columns to create songs table\n",
    "songs_table = df.select('song_id','title','artist_id','year','duration')\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.parquet(output_data + 'songs', mode='overwrite')\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_table = df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude').dropDuplicates()\n",
    "\n",
    "# write artists table to parquet files\n",
    "artists_table.write.parquet(output_data + 'artists', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------------------------------------+------------------+----+---------+\n",
      "|song_id           |title                                               |artist_id         |year|duration |\n",
      "+------------------+----------------------------------------------------+------------------+----+---------+\n",
      "|SOBAYLL12A8C138AF9|Sono andati? Fingevo di dormire                     |ARDR4AC1187FB371A1|0   |511.16363|\n",
      "|SOOLYAZ12A6701F4A6|Laws Patrolling (Album Version)                     |AREBBGV1187FB523D2|0   |173.66159|\n",
      "|SOBBUGU12A8C13E95D|Setting Fire to Sleeping Giants                     |ARMAC4T1187FB3FA4C|2004|207.77751|\n",
      "|SOAOIBZ12AB01815BE|I Hold Your Hand In Mine [Live At Royal Albert Hall]|ARPBNLO1187FB3D52F|2000|43.36281 |\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife Is Running Around On Me (Taco Hell) |ARDNS031187B9924F0|2005|186.48771|\n",
      "+------------------+----------------------------------------------------+------------------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(output_data + 'songs').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|         artist_id|         artist_name|     artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|ARNF6401187FB57032|   Sophie B. Hawkins|New York, NY [Man...|       40.79086|       -73.96644|\n",
      "|AROUOZZ1187B9ABE51|         Willie Bobo|New York, NY [Spa...|       40.79195|       -73.94512|\n",
      "|AREBBGV1187FB523D2|Mike Jones (Featu...|         Houston, TX|           null|            null|\n",
      "|ARD842G1187B997376|          Blue Rodeo|Toronto, Ontario,...|       43.64856|       -79.38533|\n",
      "|ARDR4AC1187FB371A1|Montserrat Caball...|                    |           null|            null|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(output_data + 'artists').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = input_data + 'log-data/*.json'\n",
    "\n",
    "# read log data file\n",
    "df = spark.read.json(log_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+------+-----+\n",
      "|userId|firstName| lastName|gender|level|\n",
      "+------+---------+---------+------+-----+\n",
      "|    57|Katherine|      Gay|     F| free|\n",
      "|    84|  Shakira|     Hunt|     F| free|\n",
      "|    22|     Sean|   Wilson|     F| free|\n",
      "|    52| Theodore|    Smith|     M| free|\n",
      "|    80|    Tegan|   Levine|     F| paid|\n",
      "|    15|     Lily|     Koch|     F| paid|\n",
      "|    37|   Jordan|    Hicks|     F| free|\n",
      "|    98|   Jordyn|   Powell|     F| free|\n",
      "|    48|   Marina|   Sutton|     F| free|\n",
      "|    17| Makinley|    Jones|     F| free|\n",
      "|    45| Dominick|   Norris|     M| free|\n",
      "|    43|   Jahiem|    Miles|     M| free|\n",
      "|    21|  Preston|  Sanders|     M| free|\n",
      "|     3|    Isaac|   Valdez|     M| free|\n",
      "|    62|   Connar|   Moreno|     M| free|\n",
      "|     5|   Elijah|    Davis|     M| free|\n",
      "|    44|   Aleena|    Kirby|     F| paid|\n",
      "|    50|      Ava| Robinson|     F| free|\n",
      "|    68|   Jordan|Rodriguez|     F| free|\n",
      "|    56|   Cienna|  Freeman|     F| free|\n",
      "+------+---------+---------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract columns for users table    \n",
    "users_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level').dropDuplicates()\n",
    "users_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "users_table.write.parquet(output_data + 'users', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+------+-----+\n",
      "|userId|firstName| lastName|gender|level|\n",
      "+------+---------+---------+------+-----+\n",
      "|    88| Mohammad|Rodriguez|     M| free|\n",
      "|    88| Mohammad|Rodriguez|     M| paid|\n",
      "|      |     null|     null|  null| free|\n",
      "|    11|Christian|   Porter|     F| free|\n",
      "|    75|   Joseph|Gutierrez|     M| free|\n",
      "+------+---------+---------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(output_data + 'users').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')\n",
    "df = df.withColumn('timestamp', (df.ts/1000).cast(dataType=t.TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table = df.select(col('timestamp').alias('start_time'),\n",
    "                           hour('timestamp').alias('hour'), \n",
    "                           dayofmonth('timestamp').alias('day'),\n",
    "                           weekofyear('timestamp').alias('week'),\n",
    "                           month('timestamp').alias('month'),\n",
    "                           year('timestamp').alias('year'),\n",
    "                           dayofweek('timestamp').alias('weekday'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+---+----+-----+----+-------+\n",
      "|start_time             |hour|day|week|month|year|weekday|\n",
      "+-----------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-15 00:30:26.796|0   |15 |46  |11   |2018|5      |\n",
      "|2018-11-15 00:41:21.796|0   |15 |46  |11   |2018|5      |\n",
      "|2018-11-15 00:45:41.796|0   |15 |46  |11   |2018|5      |\n",
      "|2018-11-15 03:44:09.796|3   |15 |46  |11   |2018|5      |\n",
      "|2018-11-15 05:48:55.796|5   |15 |46  |11   |2018|5      |\n",
      "+-----------------------+----+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table.write.partitionBy('year', 'month').parquet(output_data + 'time', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+---+----+-----+----+-------+\n",
      "|start_time             |hour|day|week|month|year|weekday|\n",
      "+-----------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-15 00:30:26.796|0   |15 |46  |11   |2018|5      |\n",
      "|2018-11-15 00:41:21.796|0   |15 |46  |11   |2018|5      |\n",
      "|2018-11-15 00:45:41.796|0   |15 |46  |11   |2018|5      |\n",
      "|2018-11-15 03:44:09.796|3   |15 |46  |11   |2018|5      |\n",
      "|2018-11-15 05:48:55.796|5   |15 |46  |11   |2018|5      |\n",
      "+-----------------------+----+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(output_data + 'time').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_song = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df.join(df_song, (df.song==df_song.title) & (df.artist==df_song.artist_name) & (df.length==df_song.duration), 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = df_join.select('timestamp', 'userId', 'level', 'song_id', 'artist_id', 'sessionId', 'location', 'userAgent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.write.partitionBy('year', 'month').parquet(output_data + 'songplays', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+-------+---------+---------+--------------------+--------------------+\n",
      "|           timestamp|userId|level|song_id|artist_id|sessionId|            location|           userAgent|\n",
      "+--------------------+------+-----+-------+---------+---------+--------------------+--------------------+\n",
      "|2018-11-15 00:30:...|    26| free|   null|     null|      583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n",
      "|2018-11-15 00:41:...|    26| free|   null|     null|      583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n",
      "|2018-11-15 00:45:...|    26| free|   null|     null|      583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n",
      "|2018-11-15 03:44:...|    61| free|   null|     null|      597|Houston-The Woodl...|\"Mozilla/5.0 (Mac...|\n",
      "|2018-11-15 05:48:...|    80| paid|   null|     null|      602|Portland-South Po...|\"Mozilla/5.0 (Mac...|\n",
      "+--------------------+------+-----+-------+---------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(output_data + 'songplays').show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
